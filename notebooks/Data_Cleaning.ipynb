{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "f7e9aa0a9fe1ceb3c33c5aa0b358d6952771ff9c315b2715a747dc81bd71c7e9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pyarrow.feather as feather\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "RSEED= 42\n",
    "from pysentimiento import SentimentAnalyzer\n",
    "analyzer = SentimentAnalyzer(lang=\"es\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read json files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "alert_df=pd.read_json(\"../data/alerts_cleaned.json\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corazon_df= pd.read_json(\"../data/notification-labels-to-alert-surrogate-ids.json\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "notification_df=pd.read_json(\"../data/notifications.json\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Frame Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text managing "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python -m spacy download es_core_news_lg\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import es_core_news_lg\n",
    "nlp = es_core_news_lg.load()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import emoji\n",
    "def get_emoji(x):\n",
    "        \"\"\" take the emoji of the text and append it in a column \"\"\"\n",
    "    l=[]\n",
    "    for i in nlp(x):\n",
    "        i=i.orth_\n",
    "        if i in emoji.UNICODE_EMOJI_SPANISH : \n",
    "            l.append(i)\n",
    "    return l "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def rid_emoji(x,y):\n",
    "     \"\"\" take the emoji of the column to get \n",
    "     rid of the emoji in new text column.\"\"\"\n",
    "    l=[]\n",
    "    for i in nlp(x):\n",
    "        i=i.orth_\n",
    "        if i in y:\n",
    "            pass\n",
    "        else: \n",
    "            l.append(i)\n",
    "    return ' '.join(l)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def slang_sep(x):\n",
    "         \"\"\" take the emoji of the column to get \n",
    "     rid of the emoji in new text column.\"\"\"\n",
    "    if '?' in x:\n",
    "        if x.count(\"?\")>1:\n",
    "            x=re.compile(r'[?|!]').split(x,x.count('?'))\n",
    "            return [\"\".join(x[0]+ x[1]),x[2]]\n",
    "        else:return re.compile(r'[?|!]').split(x,1)\n",
    "    elif '!' in x:return re.compile(r'[?|!]').split(x,1) \n",
    "    else:  return ['',x]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def label_text (x,y):\n",
    "    \"\"\" based on the NLP analize, get the count of grammatical annotations \n",
    "    Part Of Speech Tagging - assigning grammatical annotations.\"\"\"\n",
    "    c=0\n",
    "    for token in  nlp(x):\n",
    "        if token.tag_== y: c=c+1\n",
    "        else: pass  \n",
    "    return c  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def text_rev (x):\n",
    "    for token in  nlp(x):\n",
    "        print([token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_,\n",
    "                 token.is_alpha, token.is_stop])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def words_count (x):\n",
    "   return len(nlp(x))\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dividing the message in parts "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "alert_df['description']     = alert_df['description'].astype('string')\n",
    "alert_df['lenght']          = alert_df['description'].str.len()\n",
    "alert_df[\"emoji\"]           = alert_df[\"description\"].apply(lambda x: get_emoji(x))\n",
    "alert_df[\"emoji_size\"]      = alert_df[\"emoji\"].str.len()\n",
    "alert_df[\"txt_description\"] = alert_df.apply(lambda x: rid_emoji(x.description,x.emoji),axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "alert_df[\"slang\"] = alert_df[\"txt_description\"].apply(lambda x: slang_sep(x))\n",
    "alert_df[['slang','information']] = pd.DataFrame(alert_df[\"slang\"].tolist(), index= alert_df.index)\n",
    "alert_df.drop([\"txt_description\"],axis=1, inplace= True)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Applying Part Of Speech Tagging - assigning grammatical annotations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "alert_df[\"slang_char\"] = alert_df[\"slang\"].str.len()\n",
    "alert_df[\"slang_verb\"] = alert_df[\"slang\"].apply(lambda x:  label_text (x,'VERB'  ))\n",
    "alert_df[\"slang_pron\"] = alert_df[\"slang\"].apply(lambda x:  label_text (x,'PROPN' ))\n",
    "alert_df[\"slang_adp\" ] = alert_df[\"slang\"].apply(lambda x:  label_text (x,'ADP'   ))\n",
    "alert_df[\"slang_noun\"] = alert_df[\"slang\"].apply(lambda x:  label_text (x,'NOUN'  ))\n",
    "alert_df[\"slang_num\" ] = alert_df[\"slang\"].apply(lambda x:  label_text (x,'NUM'   ))\n",
    "alert_df[\"slang_punt\"] = alert_df[\"slang\"].apply(lambda x:  label_text (x,'PUNCT' ))\n",
    "alert_df[\"slang_det\" ] = alert_df[\"slang\"].apply(lambda x:  label_text (x,'DET'   ))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "alert_df[\"info_char\" ]  = alert_df[\"information\"].str.len()\n",
    "alert_df[\"info_words\"]  = alert_df[\"information\"].apply(lambda x:  words_count (x))\n",
    "alert_df[\"info_verb\" ]  = alert_df[\"information\"].apply(lambda x:  label_text  (x,'VERB'  ))\n",
    "alert_df[\"info_pron\" ]  = alert_df[\"information\"].apply(lambda x:  label_text  (x,'PROPN' ))\n",
    "alert_df[\"info_adp\"  ]  = alert_df[\"information\"].apply(lambda x:  label_text  (x,'ADP'   ))\n",
    "alert_df[\"info_noun\" ]  = alert_df[\"information\"].apply(lambda x:  label_text  (x,'NOUN'  ))\n",
    "alert_df[\"info_num\"  ]  = alert_df[\"information\"].apply(lambda x:  label_text  (x,'NUM'   ))\n",
    "alert_df[\"info_punt\" ]  = alert_df[\"information\"].apply(lambda x:  label_text  (x,'PUNCT' ))\n",
    "alert_df[\"info_det\"  ]  = alert_df[\"information\"].apply(lambda x:  label_text  (x,'DET'   ))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentiment analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def DescriptionSentimentAnalyzerPredict(x):\n",
    "        \"\"\" based on the NLP analize, get how positive, neutral and negative the messages are.\"\"\"\n",
    "    pred= analyzer.predict(x)\n",
    "    return analyzer.predict(x)\n",
    "\n",
    "alert_df[\"information_sentiment\"] = alert_df[\"information\"].apply(lambda x: DescriptionSentimentAnalyzerPredict(x))\n",
    "#alert_df[\"information_sentiment_output\"] = alert_df[\"description_sentiment\"].apply(lambda x: x.output)\n",
    "alert_df[\"information_sentiment_negative\"] = alert_df[\"information_sentiment\"].apply(lambda x: x.probas['NEG'])\n",
    "alert_df[\"information_sentiment_neutral\"] = alert_df[\"information_sentiment\"].apply(lambda x: x.probas['NEU'])\n",
    "alert_df[\"information_sentiment_positive\"] = alert_df[\"information_sentiment\"].apply(lambda x: x.probas['POS'])\n",
    "alert_df.drop(['information_sentiment'],axis=1, inplace=True)\n",
    "\n",
    "alert_df[\"slang_sentiment\"] = alert_df[\"slang\"].apply(lambda x: DescriptionSentimentAnalyzerPredict(x))\n",
    "#alert_df[\"slang_sentiment_output\"] = alert_df[\"slang_sentiment\"].apply(lambda x: x.output)\n",
    "alert_df[\"slang_sentiment_negative\"] = alert_df[\"slang_sentiment\"].apply(lambda x: x.probas['NEG'])\n",
    "alert_df[\"slang_sentiment_neutral\"] = alert_df[\"slang_sentiment\"].apply(lambda x: x.probas['NEU'])\n",
    "alert_df[\"slang_sentiment_positive\"] = alert_df[\"slang_sentiment\"].apply(lambda x: x.probas['POS'])\n",
    "alert_df.drop(['slang_sentiment'],axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merging alert df with agency dummy variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "alert2= alert_df[['surrogate_id','agency']].set_index('surrogate_id')\n",
    "alert2= pd.get_dummies(alert2)\n",
    "grouped_alert= alert2.groupby(by='surrogate_id').sum()\n",
    "alert_df.drop(['agency'], axis=1, inplace=True)\n",
    "alert_df.drop_duplicates(subset='surrogate_id',inplace=True,ignore_index=True)\n",
    "alert_df= pd.merge(alert_df,grouped_alert, on='surrogate_id')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merging notification df with corazon df"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "notification_df.drop_duplicates(inplace=True,ignore_index=True)\n",
    "corazon_df.drop(['day'], axis=1, inplace=True)\n",
    "corazon_df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "real_merge= notification_df.merge(corazon_df, how=\"left\", left_on=\"join_key_value\", right_on=\"notification_label_id\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merging all df together"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Queried df for proper merging\n",
    "label_merger= real_merge.query(\"join_field == 'label'\")\n",
    "alert_id_merger= real_merge.query(\"join_field == 'alertId'\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clean_df=label_merger.merge(alert_df, how=\"left\",left_on=\"corazon_surrogate_id\", right_on=\"surrogate_id\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clean_df_3=alert_id_merger.merge(alert_df, how=\"left\",left_on=\"join_key_value\", right_on=\"document_id\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An extra level of merging due to some data issues"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clean_df_2=clean_df[clean_df['description'].isnull()]\n",
    "clean_df.dropna(subset=['description'],inplace=True)\n",
    "clean_df_2=clean_df_2[[\"event_date\",\"event_timestamp\", \"event_name\",\"user_id\",\"join_field\",\"join_key_value\"]]\n",
    "clean_df_4=clean_df_2.merge(alert_df, how=\"left\",left_on=\"join_key_value\", right_on=\"document_id\")\n",
    "clean_df_4.dropna(subset=['created_at'],inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "clean_df_3.dropna(subset=['description'],inplace=True)\n",
    "almost_finished_df= pd.concat([clean_df,clean_df_4], axis=0)\n",
    "finished_df= pd.concat([almost_finished_df,clean_df_3], axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dropping irrelevant columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_df=finished_df.drop(['event_date','join_field','join_key_value','is_global','corazon_surrogate_id','notification_label_id'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fixing the event_name column:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_df[\"event_name\"].replace(\"notification_receive\",\"notification_received\", inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_df[\"event_name\"].replace(\"notification_open\",\"notification_opened\", inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature engineering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_df[\"event_timestamp\"]=pd.to_datetime(export_df[\"event_timestamp\"]*1000, unit=\"ns\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_df[\"notif_viewed_ontime\"]=list(map(lambda x,y,z: np.nan if x==\"notification_received\" else (1 if y < z else 0), export_df[\"event_name\"], export_df[\"event_timestamp\"],export_df[\"closed_at\"])) \n",
    "#1 if the user views a notification on time, 0 if not, null if does not apply. "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_df.dropna (subset=['notif_viewed_ontime'], inplace=True)\n",
    "export_df.reset_index(drop=True, inplace=True)\n",
    "export_df[\"notif_viewed_ontime\"]=export_df[\"notif_viewed_ontime\"].astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Calculation of reaction time in minutes, with small differences for notifications seen on time and not on time. \n",
    "export_df[\"reaction_time\"]=list(map(lambda x,y,z,w: float(pd.Timedelta(y - z).seconds/60)\n",
    "if (x==1) else float(pd.Timedelta(y - w).seconds/60), \n",
    "export_df[\"notif_viewed_ontime\"],export_df[\"event_timestamp\"],export_df[\"created_at\"],export_df[\"closed_at\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Combing all interaction with notifications- 0 for dismissed,\n",
    "# 1 for positive interactions (opened,share, or view alternatives)\n",
    "export_df.loc[export_df['event_name'].isin(['notification_opened', 'notification_view_alternatives' ,  'notification_share']),'opened']= 1\n",
    "export_df.loc[export_df['event_name'].isin(['notification_received', 'notification_dismiss']),'opened']= 0\n",
    "export_df['opened']=export_df['opened'].astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Grouping users in order to remove those with very few messages (less than 3).\n",
    "gpbyuser= export_df.groupby( by= ['user_id'])\n",
    "gpbyuser2=gpbyuser.sum()\n",
    "gpbyuser2['count1'] = gpbyuser.size()\n",
    "gpbyuser2.drop(gpbyuser2[gpbyuser2['count1'] <3 ].index, inplace=True)\n",
    "real_users=gpbyuser2.index.tolist()\n",
    "export_df=export_df[export_df['user_id'].isin(real_users)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Grouping by incident to create the opened rate per incident, previous step for getting our target\n",
    "gpbyincident= export_df.groupby(by= ['document_id'])\n",
    "gpbyincident2=gpbyincident.sum()\n",
    "gpbyincident2['count1'] = gpbyincident.size()\n",
    "gpbyincident2['opened_rate'] = gpbyincident2['opened']/gpbyincident2['count1']\n",
    "merge_column=gpbyincident2['opened_rate'].copy()\n",
    "export_df= export_df.merge(merge_column, left_on=\"document_id\", right_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Creating interesting message (5% of opening rate), our target (according to a previous research\n",
    "# it represents the 25% upper percentile)\n",
    "export_df[\"interesting_message\"]=list(map(lambda x: 1 if x>0.05 else 0, export_df[\"opened_rate\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Renaming agencies\n",
    "export_df.rename(columns={'agency_GewRJAw5tUmC4Ku4AX1-SQ':'Sendero_Segura','agency_GtvOEQAFZ0GtU6u4AXwvPg':'Red_Transporte_Pasajeros',\n",
    "'agency_HE59N3RXM0q5vKu4AXlQZg':'Mexibus', 'agency_JUR9bFXmVkWDHqu4AXaY0g':'Metro', 'agency_JfA8Bw8Zp024Kqu4AXiSpQ':'Metrobus',\n",
    "'agency_MgUq5b9mOEunx6u4AXt_BA':'Mexicable', 'agency_NuuRQ2I1Q0a50Kv-AVKlLA':'Trolebus','agency_V2AIQQKgmUO3VazvAOA-Cw':'Cablebus',\n",
    "'agency_jLjibFoim0iwWau4AWoEdQ':'Tren_Suburbano', 'agency_pky7jovXYkaw-awAAMrQ3g':'Tren_Ligero', 'agency_zCy9zG00HEqGeKu4AWZYNQ':'Camion_Microbus_Combi',\n",
    "}, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exporting the df for EDA."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_df.reset_index(inplace=True, drop=True)\n",
    "feather.write_feather(export_df, \"../data/data_EDA.feather\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the model df to export"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_df=pd.get_dummies(data=export_df,columns= [\"cause\",\"effect\"],drop_first=True)\n",
    "export_df.drop(['event_timestamp', 'event_name', 'user_id', 'document_id','surrogate_id', 'created_at', 'published_at', \n",
    "'closed_at','notif_viewed_ontime', 'reaction_time','area_of_effect_coordinates_latitude','area_of_effect_coordinates_longitude',\n",
    "'opened','opened_rate','description','Sendero_Segura','Red_Transporte_Pasajeros', 'Mexibus', 'Metro', 'Metrobus','Mexicable',\n",
    "'Trolebus','Tren_Suburbano', 'Tren_Ligero','Camion_Microbus_Combi','Cablebus','emoji','slang','information'],axis=1, inplace=True) #"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_df.reset_index(inplace=True, drop=True)\n",
    "feather.write_feather(export_df, \"../data/cleaned_data.feather\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}